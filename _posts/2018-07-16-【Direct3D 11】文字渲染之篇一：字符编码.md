---
layout:     page
title:      【Direct3D 11】文字渲染之篇一：字符编码
subtitle:   Unicode, UTF-8, UTF-16, UTF-32
date:       2018-08-5
author:     翼
header-img: image/bg.jpg
catalog: true
tags:
---
## 前言

>FreeType中load glyph image时，需要根据字符编码获取到字形索引(FT_Get_Char_Index)，因此这里解释一下字符编码相关的东西。这里参考了维基百科和阮一峰老师的文章（[字符编码笔记](http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html)）。

## 字符集
像字面意义一样，字符集就是字符的集合。每种字符集，定义了一系列字符，并且给字符集里面的每个字符分配了一个**码位**(**code point**)。  
比如：  
ASCII字符集，给字符'A'定的码位是0x41，给字符'B'定的码位是0x42。  
Unicode字符集，给字符'A'定的码位是U+0041，给字符'B'的码位是U+0042，给汉字“严”定的码位是U+4E25。  
所谓的“码位”，其实就是一个数值。  

## 字符编码  
字符集规定了它里面所有字符的码位，但是，没有规定码位的存储。以字符串"Hello"为例，它对应的五个字符的码位：  
U+0048 U+0065 U+006C U+006C U+006F  
这五个码位（字符串）怎么存储在内存或者磁盘中？这涉及两个问题：一个是**编码**，一个是**大小端**。  
大小端问题影响的是一个字节序列存储顺序的问题，不属于字符编码范围，这里不讲。  

字符编码，就是设计一个规则，这个规则应用到字符集中的每个字符对应的码位上，能得到一个字节序列。

## Unicode
Unicode就是一个字符集，它规定了这个字符集里面的各个字符的码位是多少。  
**请注意**，Unicode规定了每个字符的码位是多少，但是，码位在计算机里存储的形式是二进制值（其实一切都是二进制值）。而Unicode并没有规定这些码位数值应该存储成什么样的二进制值。  
举例来说，汉字“严”的Unicode码位是20005，十六进制表示是0x4E25。要把这一个数值存储到计算机里面，可以有N中方式：
1. 以2个字节存储： ‭0100 1110 0010 0101‬
2. 以3个字节存储： 0000 0000 ‭0100 1110 0010 0101‬
3. 以4个字节存储： 0000 0000 0000 0000 ‭0100 1110 0010 0101‬
4. 以N个字节存储（当然，实际方案不可能这么浪费，也不可能这么简单）。

这就是问题了，Unicode标准没有规定0x4E25应该怎么存储。  
而实际上，Unicode包含那么多字符，他们对应的那么多码位，有些类似0x4E25可以两个字节就足够存储，有些需要三个字节，或一个字节就够。这些问题，都是Unicode本身并没有规定的。  

另外提一下，简体中文的编码GB2312等，和Unicode是没什么关系的。

## UTF-8, UTF-16, UTF-32
如何存储Unicode码位，不同的实现方式（存储方式）衍生出了UTF-8, UTF-16, UTF-32。  

### UTF-8
UTF-8 是在互联网上使用最广的**一种 Unicode 的实现方式**。  
UTF-8 最大的一个特点，就是它是**一种变长的编码方式**。它可以使用1~4个字节表示一个符号，根据不同的符号的码位而变化字节长度。  
UTF-8 的存储规则很简单，只有二条：  
1. 对于单字节的码位，字节的第一位设为0，后面7位为这个符号的 Unicode 码。因此对于英文字母，UTF-8 编码和 ASCII 码是相同的。
2. 对于n字节的符号码位（n > 1），第一个字节的前n位都设为1，第n + 1位设为0，后面字节的前两位一律设为 10 。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码位。

Unicode符号范围(十六进制)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UTF-8编码方式（二进制）  
------------------------------------------+---------------------------------------------  
0000 0000 - 0000 007F    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;  0xxxxxxx  
0000 0080 - 0000 07FF    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;  110xxxxx 10xxxxxx  
0000 0800 - 0000 FFFF    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;  1110xxxx 10xxxxxx 10xxxxxx  
0001 0000 - 0010 FFFF    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;  11110xxx 10xxxxxx 10xxxxxx 10xxxxxx  

跟据上表，解读 UTF-8 编码非常简单。如果一个字节的第一位是0，则这个字节单独就是一个字符；如果第一位是1，则连续有多少个1，就表示当前字符占用多少个字节。  
还是以汉字“严”为例，演示如何实现 UTF-8 编码。  
“严”的 Unicode 编码是0x4E25（100111000100101），根据上表，可以发现0x4E25处在第三行的范围内（0000 0800 - 0000 FFFF），
因此“严”的 UTF-8 存储需要三个字节，即格式是1110xxxx 10xxxxxx 10xxxxxx。然后，从严的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。
这样就得到了，“严”的 UTF-8 编码：11100100 10111000 10100101，转换成十六进制就是0xE4B8A5。

可见UTF-8 以8 bits 为一个存储单元，用 1-4 个单元存储一个Unicode编码。  

### UTF-16，UTF-32
UTF-16，UTF-32 和 UTF-8 类似，只不过：  
UTF-16 以 16 bits 为一个存储单元，用 1-2 个单元存储一个Unicode编码。  
UTF-32 以 32 bits 为一个存储单元，用 1 个单元存储一个Unicode编码。

## std::string std::wstring 和 编码
可以认为std::string是一个char序列，std::wstring是一个wchar_t序列。  
char和wchar_t只是C++的一种用于存储数据的类型，至于数据是什么，是看我们怎么解释数据的（毕竟，说到底都是一堆二进制）。  
问题来了，如果char和wchar_t与编码无关，那std::string和std::wstring也与编码无关了。那么VS调试时还能正确看到它们存储的字符串呢？  
答案是，上面说过，它们存的是什么，是看我们怎么解释它们的数据的，在这里，就是VS在解释那些数据。  
举例如下：  
```
    std::string s1 = "中国";
    const char* ps1 = s1.c_str();

    std::wstring s2 = L"中国";
    const wchar_t* ps2 = s2.c_str();
    
    std::string s3 = CW2AEX<>(s2.c_str(), CP_UTF8);
    const char* ps3 = s3.c_str();

```
在这段代码中，分别用std::string和std::wstring存储了字符串“中国”，查看对应内存的内容：  
```
    ps1:
    
    0x012FF8D8  d6 d0 b9 fa ? ? ? ?
    0x012FF8DC  00 cc cc cc  . ? ? ?
    
    ps2:
    
    0x012FF8A8  2d 4e fd 56 - N ? V
    0x012FF8AC  00 00 cc cc  .. ? ?
    
    ps3:
    
    0x012FF878  e4 b8 ad e5 ? ? ? ?
    0x012FF87C  9b bd 00 cc ? ? . ?
```
可见，s1,s2,s3存储的东西都不一样。下面就来解释原因。  
1. 对于s1，因为它是一个std::string, 编译器会以ANSI编码方式来把“中国”的编码值存入s1。
   而在简体中文Windows操作系统中，ANSI 编码代表 GBK 编码。“中国”的GBK编码正是\xD6\xD0 \xB9\xFA。
   编译器把字节序列 \xD6 \xD0 \xB9 \xFA 存入std::string中。至于ps1指向的内存中最后的0，是c_str()添加的。
2. 对于s2，因为是std::wstring，编译器会以UTF-16编码“中国”（不论工程配置是选择Unicode字符集还是多字节），并存入s2中。
   而“中国”的UTF-16编码是U+4E2D U+56FD，所以编译器把两个wchar_t类型的值\x4E\x2D和\x56\xFD存入std::wstring中。因为大小端问题，
   wchar_t的高低两个字节顺序才会如上面内存映像所示。ps2指向的内存的结尾的两个字节0，也是c_str()添加的。
3. 对于s3，“中国”的UTF-8编码正是：\xE4 \xB8 \xAD \xE5 \x9B \xBD。

上面很详细的说明了，std::string和std::wstring本身，并不涉及编码，它们正是一种数据类型。
特别是std::string，可以简单地理解为std::string就是存储字节数据的。我经常用std::string存储和传递字节数据，比如图像。


### 后记
总结一下就是：Unicode是一个**字符集**。而 UTF-8, UTF-16, UTF-32 等等都是该字符集的编码方式，或者说他们明确了Unicode字符的码位的**字节序列**。  
```
1. UTF-8  : one to four 8-bit units
2. UTF-16 : one or two 16-bit code units
3. UTF-32 : a single 32-bit code unit
```

上面说的只是编码字节序列，实际存储的时候，还有大小端问题。不过不影响我们理解Unicode和UTF-8等之间的关系了，在此不提。
